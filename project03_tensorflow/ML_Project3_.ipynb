{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L48ILuwMhJ7R"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TMuN3wjZofkj"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zipfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d72d5ce98a26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/gzip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gzip.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gzip.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d72d5ce98a26>\u001b[0m in \u001b[0;36mextract\u001b[0;34m(gzip_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mzip_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'zipfile' is not defined"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "\n",
    "def download_file(url, file_name):\n",
    "    with open('gzip.zip', \"wb\") as file:\n",
    "        response = get(url)\n",
    "        file.write(response.content)\n",
    "\n",
    "download_file('http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip', 'gzip.zip')\n",
    "\n",
    "#Extract Files \n",
    "\n",
    "\n",
    "\n",
    "def extract(gzip_path):\n",
    "    zip_ref = zipfile.ZipFile(gzip_path, 'r')\n",
    "    zip_ref.extractall('datasets')\n",
    "    zip_ref.close()\n",
    "    for file in glob.glob(r'datasets/gzip/emnist-byclass*'):\n",
    "        shutil.move(file, 'datasets/')\n",
    "    shutil.rmtree('datasets/gzip')\n",
    "    os.remove('gzip.zip')\n",
    "extract('gzip.zip')\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def read_emnist(images_path: str, labels_path: str):\n",
    "    with gzip.open(labels_path, 'rb') as labelsFile:\n",
    "        labels = np.frombuffer(labelsFile.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(images_path,'rb') as imagesFile:\n",
    "        length = len(labels)\n",
    "        # Load flat 28x28 px images (784 px), and convert them to 28x28 px\n",
    "        features = np.frombuffer(imagesFile.read(), dtype=np.uint8, offset=16) \\\n",
    "                        .reshape(length, 784) \\\n",
    "                        .reshape(length, 28, 28, 1)\n",
    "        \n",
    "    return features, labels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nrZKG-ldvVS"
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vt8sGLybofkl"
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_emnist(images_path: str, labels_path: str):\n",
    "    with gzip.open(labels_path, 'rb') as labelsFile:\n",
    "        labels = np.frombuffer(labelsFile.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(images_path,'rb') as imagesFile:\n",
    "        length = len(labels)\n",
    "        # Load flat 28x28 px images (784 px), and convert them to 28x28 px\n",
    "        features = np.frombuffer(imagesFile.read(), dtype=np.uint8, offset=16) \\\n",
    "                        .reshape(length, 784) \\\n",
    "                        .reshape(length, 28, 28, 1)\n",
    "        \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrC6FnfZofkm"
   },
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "\n",
    "train['features'], train['labels'] = read_emnist('datasets/emnist-byclass-train-images-idx3-ubyte.gz', 'datasets/emnist-byclass-train-labels-idx1-ubyte.gz')\n",
    "test['features'], test['labels'] = read_emnist('datasets/emnist-byclass-test-images-idx3-ubyte.gz', 'datasets/emnist-byclass-test-labels-idx1-ubyte.gz')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUG7dNtlofkm"
   },
   "outputs": [],
   "source": [
    "def rotate(img):\n",
    "    # Used to rotate images (for some reason they are transposed on read-in)\n",
    "    flipped = np.fliplr(img)\n",
    "    return np.rot90(flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tejllvMBofkm"
   },
   "outputs": [],
   "source": [
    "# fix the orientation of the images\n",
    "train['features'] = np.array(list(map(rotate, train['features'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVK-bAFIofkn"
   },
   "outputs": [],
   "source": [
    "test['features'] = np.array(list(map(rotate, test['features'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ct4tQTqofkn"
   },
   "outputs": [],
   "source": [
    "# Get the ascii label\n",
    "label_map = {}\n",
    "def load_label_map(map_path):\n",
    "    with open(map_path, 'r') as map_file:\n",
    "        for line in map_file:\n",
    "            mapping = line.split()\n",
    "            label_map[int(mapping[0])] = chr(int(mapping[1]))\n",
    "load_label_map('datasets/emnist-byclass-mapping.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3xo7rZcofkn"
   },
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrPwFZVaofko"
   },
   "outputs": [],
   "source": [
    "def display_image(mode, position):\n",
    "    if mode == 'train':\n",
    "        image = train['features'][position].squeeze()\n",
    "        plt.title('%d. Label: %s' % (position, label_map[train['labels'][position]]))\n",
    "    else:\n",
    "        image = test['features'][position].squeeze()\n",
    "        plt.title('%d. Label: %s' % (position, label_map[test['labels'][position]]))\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrwIKO2-ofko"
   },
   "outputs": [],
   "source": [
    "display_image('train', 1210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQYK8Aerofkp"
   },
   "outputs": [],
   "source": [
    "display_image('test', 1001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvLqCOrfofkp"
   },
   "source": [
    "### Training data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "id": "swUnH2aMofkq",
    "outputId": "202d69eb-0412-4428-d7b6-9ef084de2693",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_labels_count = np.unique(train['labels'], return_counts=True)\n",
    "dataframe_train_labels = pd.DataFrame({'Label':train_labels_count[0], 'Count':train_labels_count[1]})\n",
    "dataframe_train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-23_ob_ofkr"
   },
   "source": [
    "### Create a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sZ3-gdFofkr"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXyAENmtofks"
   },
   "outputs": [],
   "source": [
    "validation = {}\n",
    "train['features'], validation['features'], train['labels'], validation['labels'] = train_test_split(train['features'], train['labels'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZHgla9Jofks",
    "outputId": "cc9665de-900f-41cf-c98e-978d0b9af4b3"
   },
   "outputs": [],
   "source": [
    "print('# of training images:', train['features'].shape[0])\n",
    "print('# of validation images:', validation['features'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIx2mueDofkt"
   },
   "source": [
    "### Prepare our input features (padding 28x28 to 32x32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjNNAaeDofkt",
    "outputId": "89708d7f-4a8c-4e39-ff25-a45a9fa44896"
   },
   "outputs": [],
   "source": [
    "# Pad images with 0s\n",
    "train['features']      = np.pad(train['features'], ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "validation['features'] = np.pad(validation['features'], ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "test['features']       = np.pad(test['features'], ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    \n",
    "print(\"Updated Image Shape: {}\".format(train['features'][0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-CiCk5pofkt"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "## Build the Custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4k6vzIG5ofku"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# input shape is 32,32,1\n",
    "# The custom model is a Deep CNN with 2 conv layers and 2 hidden layers \n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "   \n",
    "    # 2 Convolutional Layers \n",
    "\n",
    "    #The first \n",
    "    model.add(Conv2D(filters=6, kernel_size=(3,3), activation='relu', input_shape= (32,32,1)))\n",
    "    model.add(AveragePooling2D()) # Average Pooling for subsampling \n",
    "    \n",
    "    # Second \n",
    "    model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(AveragePooling2D()) # Average Pooling for subsampling \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # 2 Hidden Layers \n",
    "    model.add(Dense(units=120, activation='relu'))\n",
    "    model.add(Dense(units=84, activation='relu'))\n",
    "\n",
    "    #Output Layer\n",
    "    model.add(Dense(units=62, activation='softmax'))\n",
    "    \n",
    "    # model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    #print(model.summary())\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    #return model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1853J5brofku",
    "outputId": "a9d09200-0021-419b-efd0-7adce2f77e01"
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rj4okl_dofkv"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrCXklJwofkv"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcTYLcfhofkv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oAEBtBWofkv"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = train['features'], to_categorical(train['labels'])\n",
    "X_validation, y_validation = validation['features'], to_categorical(validation['labels'])\n",
    "\n",
    "train_generator = ImageDataGenerator().flow(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "validation_generator = ImageDataGenerator().flow(X_validation, y_validation, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LGMzoMzxofkw",
    "outputId": "8fb43ed0-e282-4106-a1f7-aee0e12faa16"
   },
   "outputs": [],
   "source": [
    "print('# of training images:', train['features'].shape[0])\n",
    "print('# of validation images:', validation['features'].shape[0])\n",
    "\n",
    "steps_per_epoch = X_train.shape[0]//BATCH_SIZE\n",
    "validation_steps = X_validation.shape[0]//BATCH_SIZE\n",
    "\n",
    "model.fit(X_train,y_train,epochs=3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0E4vJ94Tofkw"
   },
   "source": [
    "### Evaluate the trained model\n",
    "Evaluate on the test set. The model has not seen any of these examples during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OccnrwWcofkw",
    "outputId": "20b5f0ab-3e17-443c-e160-1a6fec1597df"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(test['features'], to_categorical(test['labels']))\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSA_C7Ajofkz"
   },
   "source": [
    "### Use the model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_MT6Yybofkz"
   },
   "outputs": [],
   "source": [
    "def test_the_model(position):\n",
    "    out = model.predict(np.reshape(test['features'][position], (1,32,32,1)))\n",
    "    image = test['features'][position].squeeze()\n",
    "    plt.title('I: %d. True Label: %s, Predicted: %s' % (position, label_map[test['labels'][position]], label_map[np.argmax(out)]))\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-tcbdLiofk0"
   },
   "outputs": [],
   "source": [
    "test_the_model(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hmYJdjDofk1"
   },
   "source": [
    "### Save the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWr5J68aofk1"
   },
   "outputs": [],
   "source": [
    "from keras.models import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZadzmvxofk1"
   },
   "outputs": [],
   "source": [
    "# Offload model to file\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "save_model(model, 'model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr_jSF1wofk1"
   },
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbrUOKi3ofk2"
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "def load_model():\n",
    "    ''' Load model from .yaml and the weights from .h5\n",
    "        Arguments:\n",
    "            bin_dir: The directory of the bin (normally bin/)\n",
    "        Returns:\n",
    "            Loaded model from file\n",
    "    '''\n",
    "\n",
    "    # load YAML and create model\n",
    "    yaml_file = open('model.yaml', 'r')\n",
    "    loaded_model_yaml = yaml_file.read()\n",
    "    yaml_file.close()\n",
    "    model = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "    # load weights into new model\n",
    "    model.load_weights('model.h5')\n",
    "    return model\n",
    "\n",
    "trained_model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYct9NHEofk2"
   },
   "source": [
    "### Use the trained model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0P9c3aoofk2"
   },
   "outputs": [],
   "source": [
    "def test_the_trained_model(position):\n",
    "    out = trained_model.predict(np.reshape(test['features'][position], (1,32,32,1)))\n",
    "    image = test['features'][position].squeeze()\n",
    "    plt.title('I: %d. True Label: %s, Predicted: %s' % (position, label_map[test['labels'][position]], label_map[np.argmax(out)]))\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yd33jlATofk3"
   },
   "outputs": [],
   "source": [
    "test_the_model(1002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exqnc5ZCkeqo"
   },
   "source": [
    "#Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1sUpFC_kjkz",
    "outputId": "ed3ff5ab-8829-4322-da3a-37957e818ba2"
   },
   "outputs": [],
   "source": [
    "my_estimator = tf.keras.estimator.model_to_estimator(\n",
    "    keras_model=model,\n",
    "    model_dir='models/estimator-for-XOR/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ef6vjliqD3te"
   },
   "outputs": [],
   "source": [
    "def train_input_fn(x_train, y_train, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        ({'input-features':x_train}, y_train.reshape(-1, 62)))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    return dataset.shuffle(100).repeat().batch(batch_size)\n",
    "\n",
    "def eval_input_fn(x_test, y_test=None, batch_size = 128):\n",
    "    if y_test is None:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            {'input-features':x_test})\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            ({'input-features':x_test}, y_test.reshape(-1, -1)))\n",
    "\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    return dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESYHEGEHlOjn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyUYJ-E4DYiB"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = train['features'], to_categorical(train['labels'])\n",
    "X_validation, y_validation = validation['features'], to_categorical(validation['labels'])\n",
    "\n",
    "steps_per_epoch = X_train.shape[0]//BATCH_SIZE\n",
    "\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = np.ceil(len(X_train)/BATCH_SIZE)\n",
    "\n",
    "train_input_fn(X_train, y_train, BATCH_SIZE)\n",
    "input_fn=lambda: train_input_fn(X_train, y_train, BATCH_SIZE)\n",
    "\n",
    "my_estimator.train(\n",
    "    input_fn=input_fn,\n",
    "    steps=EPOCHS*BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aClC8TfUvduK"
   },
   "outputs": [],
   "source": [
    "34617390/558345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40p9vDxFjVOZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_Project3_.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
